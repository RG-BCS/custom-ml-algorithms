# -*- coding: utf-8 -*-
"""perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBkRx3zcZqLB5_Zuq6uHAsPLXGc655Hl
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from utils import plot_decision_region

# -------------------------------------------------------
# AdalineSGD: Adaptive Linear Neuron using Stochastic Gradient Descent
# -------------------------------------------------------
class AdalineSGD:
    def __init__(self, n_iter=20, eta=0.01, random_state=1, shuffle=True):
        self.n_iter = n_iter            # Number of epochs
        self.eta = eta                  # Learning rate
        self.random_state = random_state
        self.shuffle = shuffle          # Whether to shuffle each epoch

    def fit(self, X, y):
        rng = np.random.RandomState(self.random_state)
        self.w_ = rng.normal(loc=0.0, scale=0.01, size=X.shape[1])
        self.b_ = 0.0
        self.losses_ = []

        for epoch in range(self.n_iter):
            total_loss = 0.0

            # Shuffle data each epoch (if enabled)
            if self.shuffle:
                perm = rng.permutation(len(X))
                X_shuffled, y_shuffled = X[perm], y[perm]
            else:
                X_shuffled, y_shuffled = X, y

            # Update weights per sample
            for x_sample, y_sample in zip(X_shuffled, y_shuffled):
                net_input = self.net_input(x_sample)
                activation = self.activation(net_input)
                error = y_sample - activation

                self.b_ += 2 * self.eta * error
                self.w_ += 2 * self.eta * error * x_sample
                total_loss += error**2

            self.losses_.append(total_loss / len(X))

        return self

    def partial_fit(self, X, y):
        # Online learning support (one or few samples)
        if self.shuffle and len(X) > 1:
            perm = np.random.permutation(len(X))
            X, y = X[perm], y[perm]

        for x_sample, y_sample in zip(X, y):
            net_input = self.net_input(x_sample)
            activation = self.activation(net_input)
            error = y_sample - activation

            self.b_ += 2 * self.eta * error
            self.w_ += 2 * self.eta * error * x_sample

        return self

    def net_input(self, x):
        return np.dot(x, self.w_) + self.b_

    def activation(self, x):
        return x  # Identity

    def predict(self, x):
        return np.where(self.activation(self.net_input(x)) >= 0.5, 1, 0)

    def __repr__(self):
        return (f"AdalineSGD(n_iter={self.n_iter}, eta={self.eta}, "
                f"random_state={self.random_state}, shuffle={self.shuffle})")

# -------------------------------------------------------
# Dataset Preparation: Binary Iris Classification
# -------------------------------------------------------
iris = load_iris()
mask = (iris.target == 0) | (iris.target == 1)
X = iris.data[mask][:, [0, 2]]  # Sepal length and petal length
y = iris.target[mask]

# Shuffle the dataset
perm = np.random.permutation(len(X))
X, y = X[perm], y[perm]

# Standardize features
X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)

# -------------------------------------------------------
# Train Adaline with Stochastic Gradient Descent
# -------------------------------------------------------
model = AdalineSGD(n_iter=15, eta=0.01, random_state=1)
model.fit(X_scaled, y)

# -------------------------------------------------------
# Plot Results
# -------------------------------------------------------
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
plot_decision_region(X_scaled, y, model)
plt.title('Adaline (SGD) - Decision Boundary')
plt.xlabel('Sepal Length (standardized)')
plt.ylabel('Petal Length (standardized)')
plt.legend(loc='upper left')

plt.subplot(1, 2, 2)
plt.plot(model.losses_, 'o-')
plt.title('Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Mean Squared Error')

plt.tight_layout()
plt.show()