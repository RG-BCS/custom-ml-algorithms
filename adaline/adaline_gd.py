# -*- coding: utf-8 -*-
"""perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBkRx3zcZqLB5_Zuq6uHAsPLXGc655Hl
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from utils import plot_decision_region

# -------------------------------------------------------
# AdalineGD: Adaptive Linear Neuron using Batch Gradient Descent
# -------------------------------------------------------
class AdalineGD:
    def __init__(self, n_iter=50, eta=0.01, random_state=12):
        self.n_iter = n_iter                  # Number of training epochs
        self.eta = eta                        # Learning rate
        self.random_state = random_state      # Seed for weight initialization

    def fit(self, X, y):
        # Initialize weights and bias
        rnd = np.random.RandomState(self.random_state)
        self.w_ = rnd.normal(loc=0.0, scale=0.01, size=X.shape[1])
        self.b_ = 0.0

        self.losses_ = []

        # Training loop
        for epoch in range(self.n_iter):
            z = self.input_sum(X)                 # Linear combination
            activation = self.activation(z)       # Activation (identity)
            error = y - activation                # Prediction error

            # Update weights and bias using MSE gradient
            self.b_ += 2 * self.eta * error.mean()
            self.w_ += 2 * self.eta * error.dot(X) / X.shape[0]

            # Record mean squared error for monitoring
            self.losses_.append((error ** 2).mean())

        return self

    def input_sum(self, x):
        return np.dot(x, self.w_) + self.b_

    def activation(self, x):
        return x  # Identity function

    def predict(self, x):
        # Classify based on threshold (0.5)
        return np.where(self.activation(self.input_sum(x)) >= 0.5, 1, 0)

    def loss_fn(self, x, y_true, y_pred=None):
        # Optional external call to MSE loss
        if y_pred is None:
            y_pred = self.activation(self.input_sum(x))
        return np.mean((y_true - y_pred) ** 2)

    def __repr__(self):
        return f"Adaline(n_iter={self.n_iter}, eta={self.eta}, random_state={self.random_state})"

# -------------------------------------------------------
# Dataset Preparation (Binary Iris Classification)
# -------------------------------------------------------
iris = load_iris()
mask = (iris.target == 0) | (iris.target == 1)
X = iris.data[mask][:, [0, 2]]  # Use Sepal Length and Petal Length
y = iris.target[mask]

# Shuffle the data
perm = np.random.permutation(len(X))
X, y = X[perm], y[perm]

# -------------------------------------------------------
# Training and Visualization (Before Standardization)
# -------------------------------------------------------
plt.figure(figsize=(14, 10))

ada1 = AdalineGD(eta=0.1, n_iter=15).fit(X, y)

# Plot decision boundary
plt.subplot(2, 3, 1)
plot_decision_region(X, y, ada1)
plt.title("Before Standardization")
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Petal Length (cm)')
plt.legend()

# Plot loss (log scale)
plt.subplot(2, 3, 2)
plt.plot(range(1, len(ada1.losses_) + 1), np.log10(ada1.losses_), marker='o')
plt.title(f'Adaline - Learning Rate: {ada1.eta}')
plt.xlabel('Epochs')
plt.ylabel('log(MSE)')

# Try smaller learning rate (to show slower convergence)
ada2 = AdalineGD(eta=0.0001, n_iter=15).fit(X, y)

plt.subplot(2, 3, 3)
plt.plot(range(1, len(ada2.losses_) + 1), ada2.losses_, marker='o')
plt.title(f'Adaline - Learning Rate: {ada2.eta}')
plt.xlabel('Epochs')
plt.ylabel('MSE')

# -------------------------------------------------------
# Standardize Input Features
# -------------------------------------------------------
X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)

# Retrain on standardized data
ad1 = AdalineGD(eta=0.1, n_iter=15).fit(X_scaled, y)
ad2 = AdalineGD(eta=0.0001, n_iter=15).fit(X_scaled, y)

# -------------------------------------------------------
# Visualization (After Standardization)
# -------------------------------------------------------
plt.subplot(2, 3, 4)
plot_decision_region(X_scaled, y, ad1)
plt.title("After Standardization")
plt.xlabel('Sepal Length (scaled)')
plt.ylabel('Petal Length (scaled)')
plt.legend()

plt.subplot(2, 3, 5)
plt.plot(range(1, len(ad1.losses_) + 1), np.log10(ad1.losses_), marker='o')
plt.title(f'Standardized - Learning Rate: {ad1.eta}')
plt.xlabel('Epochs')
plt.ylabel('log(MSE)')

plt.subplot(2, 3, 6)
plt.plot(range(1, len(ad2.losses_) + 1), ad2.losses_, marker='o')
plt.title(f'Standardized - Learning Rate: {ad2.eta}')
plt.xlabel('Epochs')
plt.ylabel('MSE')

plt.tight_layout()
plt.show()