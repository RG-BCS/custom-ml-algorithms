# -*- coding: utf-8 -*-
"""perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBkRx3zcZqLB5_Zuq6uHAsPLXGc655Hl
"""

import numpy as np
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

# Implementing a Perceptron from scratch
# Note: This model doesn't use a loss function in the traditional sense.
# Weight updates are based on classification errors (no gradient descent with loss derivatives).

class Perceptron:
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        self.eta = eta                      # Learning rate
        self.n_iter = n_iter                # Number of training epochs
        self.random_state = random_state    # For reproducible weight initialization

    def fit(self, X, y):
        rng = np.random.RandomState(self.random_state)
        self.w_ = rng.normal(loc=0.0, scale=0.01, size=X.shape[1])  # Weight initialization
        # Alternative: self.w_ = np.zeros(X.shape[1])  # To test learning behavior from zero init
        self.b_ = 0.0
        self.errors_ = []

        for epoch in range(self.n_iter):
            errors = 0
            for x_i, y_i in zip(X, y):
                y_pred = self.predict(x_i)
                error = y_i - y_pred
                # Update rule
                self.w_ += self.eta * error * x_i
                self.b_ += self.eta * error
                errors += int(error != 0)
            self.errors_.append(errors)
        return self

    def net_input(self, x):
        """Calculate net input (weighted sum + bias)."""
        return np.dot(x, self.w_) + self.b_

    def predict(self, x):
        """Return class label after unit step."""
        return np.where(self.net_input(x) >= 0.0, 1, 0)

    def __repr__(self):
        return (f"Perceptron(n_iter={self.n_iter}, eta={self.eta}, random_state={self.random_state})")