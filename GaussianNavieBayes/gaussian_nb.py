# -*- coding: utf-8 -*-
"""perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBkRx3zcZqLB5_Zuq6uHAsPLXGc655Hl
"""

# gaussian_nb.py

import numpy as np

class GaussianNaiveBayes_:
    """
    Custom implementation of binary Gaussian Naive Bayes classifier.
    """

    def __init__(self, eps=1e-9):
        self.eps = eps  # Small value to avoid division by zero

    def fit(self, x, y):
        """
        Fits the model to the training data.

        Parameters:
        - x (np.ndarray): Feature matrix of shape (n_samples, n_features)
        - y (np.ndarray): Binary target labels (0 or 1)
        """
        assert x.shape[0] == len(y), "Mismatch between number of samples and labels"

        self.p_0, self.p_1 = np.bincount(y) / len(y)
        self.mean_0 = np.mean(x[y == 0], axis=0)
        self.mean_1 = np.mean(x[y == 1], axis=0)
        self.var_0 = np.var(x[y == 0], axis=0, ddof=1)
        self.var_1 = np.var(x[y == 1], axis=0, ddof=1)

        return self

    def predict(self, x):
        """
        Predicts binary class labels for given input data.

        Parameters:
        - x (np.ndarray): Input features

        Returns:
        - np.ndarray: Predicted labels (0 or 1)
        """
        var_0 = self.var_0 + self.eps
        var_1 = self.var_1 + self.eps

        log_likelihood_0 = -0.5 * np.sum(np.log(2 * np.pi * var_0) + ((x - self.mean_0) ** 2) / var_0, axis=1)
        log_likelihood_1 = -0.5 * np.sum(np.log(2 * np.pi * var_1) + ((x - self.mean_1) ** 2) / var_1, axis=1)

        log_posterior_0 = np.log(self.p_0) + log_likelihood_0
        log_posterior_1 = np.log(self.p_1) + log_likelihood_1

        return (log_posterior_1 > log_posterior_0).astype(int)

    def predict_proba(self, x):
        """
        Predicts class probabilities using softmax over log posteriors.

        Parameters:
        - x (np.ndarray): Input features

        Returns:
        - np.ndarray: Class probabilities
        """
        var_0 = self.var_0 + self.eps
        var_1 = self.var_1 + self.eps

        log_likelihood_0 = -0.5 * np.sum(np.log(2 * np.pi * var_0) + ((x - self.mean_0) ** 2) / var_0, axis=1)
        log_likelihood_1 = -0.5 * np.sum(np.log(2 * np.pi * var_1) + ((x - self.mean_1) ** 2) / var_1, axis=1)

        log_posterior_0 = np.log(self.p_0) + log_likelihood_0
        log_posterior_1 = np.log(self.p_1) + log_likelihood_1

        logits = np.stack([log_posterior_0, log_posterior_1], axis=1)
        logits -= np.max(logits, axis=1, keepdims=True)
        probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)

        return probs

    def score(self, x, y):
        """
        Returns accuracy of the model on given data.

        Parameters:
        - x (np.ndarray): Input features
        - y (np.ndarray): True labels

        Returns:
        - float: Accuracy
        """
        return np.mean(self.predict(x) == y)

    def __repr__(self):
        return f"GaussianNaiveBayes_(eps={self.eps})"