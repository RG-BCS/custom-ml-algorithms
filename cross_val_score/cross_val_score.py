# -*- coding: utf-8 -*-
"""perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBkRx3zcZqLB5_Zuq6uHAsPLXGc655Hl
"""

import numpy as np

def _confusion_elements(y_true, y_pred):
    """
    Compute True Positives, False Positives, False Negatives, True Negatives for binary classification.

    Parameters:
        y_true (np.ndarray): True labels (binary: 0 or 1).
        y_pred (np.ndarray): Predicted labels (binary: 0 or 1).

    Returns:
        Tp, Fp, Fn, Tn (ints): Confusion matrix elements.
    """
    Tp = np.sum((y_true == 1) & (y_pred == 1))
    Tn = np.sum((y_true == 0) & (y_pred == 0))
    Fp = np.sum((y_true == 0) & (y_pred == 1))
    Fn = np.sum((y_true == 1) & (y_pred == 0))
    return Tp, Fp, Fn, Tn

def _accuracy_score(y_true, y_pred):
    return np.mean(y_true == y_pred)

def _precision_score(y_true, y_pred):
    Tp, Fp, _, _ = _confusion_elements(y_true, y_pred)
    return Tp / (Tp + Fp) if (Tp + Fp) > 0 else 0.0

def _recall_score(y_true, y_pred):
    Tp, _, Fn, _ = _confusion_elements(y_true, y_pred)
    return Tp / (Tp + Fn) if (Tp + Fn) > 0 else 0.0

def _f1_score(y_true, y_pred):
    prec = _precision_score(y_true, y_pred)
    reca = _recall_score(y_true, y_pred)
    return 2 * (prec * reca) / (prec + reca) if (prec + reca) > 0 else 0.0

def cross_val_score(estimator, X, y, cv=5, scoring='accuracy', random_state=None, stratified=False):
    """
    Perform k-fold cross-validation with multiple scoring options.

    Parameters:
        estimator: An object with .fit(X_train, y_train) and .predict(X_test) methods.
        X (np.ndarray): Features, shape (n_samples, n_features).
        y (np.ndarray): Targets, shape (n_samples,).
        cv (int): Number of folds.
        scoring (str): One of {'accuracy', 'precision', 'recall', 'f1-score'}.
        random_state (int or None): Seed for reproducibility.
        stratified (bool): If True, perform stratified splits (binary classification only).

    Returns:
        scores (np.ndarray): Array of scores for each fold.
    """
    assert len(X) == len(y), "Features and labels must have the same length."
    n_samples = len(y)

    if random_state is not None:
        np.random.seed(random_state)

    if stratified:
        # Only support binary classification for stratification here
        assert set(np.unique(y)) <= {0, 1}, "Stratified CV only supports binary classification for now."
        # Get indices per class
        idx_class0 = np.where(y == 0)[0]
        idx_class1 = np.where(y == 1)[0]

        # Shuffle indices
        np.random.shuffle(idx_class0)
        np.random.shuffle(idx_class1)

        folds = []
        for i in range(cv):
            test_idx_class0 = idx_class0[i * len(idx_class0) // cv:(i + 1) * len(idx_class0) // cv]
            test_idx_class1 = idx_class1[i * len(idx_class1) // cv:(i + 1) * len(idx_class1) // cv]
            test_idx = np.concatenate([test_idx_class0, test_idx_class1])
            train_idx = np.setdiff1d(np.arange(n_samples), test_idx)
            folds.append((train_idx, test_idx))
    else:
        # Non-stratified random splits
        perm = np.random.permutation(n_samples)
        fold_sizes = (n_samples // cv) * np.ones(cv, dtype=int)
        fold_sizes[:n_samples % cv] += 1
        current = 0
        folds = []
        for fold_size in fold_sizes:
            start, stop = current, current + fold_size
            test_idx = perm[start:stop]
            train_idx = np.setdiff1d(perm, test_idx)
            folds.append((train_idx, test_idx))
            current = stop

    scorer_map = {
        'accuracy': _accuracy_score,
        'precision': _precision_score,
        'recall': _recall_score,
        'f1-score': _f1_score,
    }

    if scoring not in scorer_map:
        raise ValueError(f"Scoring '{scoring}' not supported. Choose from {list(scorer_map.keys())}.")

    scores = []
    for train_idx, test_idx in folds:
        X_train, y_train = X[train_idx], y[train_idx]
        X_test, y_test = X[test_idx], y[test_idx]

        clf = estimator.fit(X_train, y_train)
        y_pred = clf.predict(X_test)

        score = scorer_map[scoring](y_test, y_pred)
        scores.append(score)

    return np.array(scores)