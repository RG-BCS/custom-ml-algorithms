# -*- coding: utf-8 -*-
"""perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBkRx3zcZqLB5_Zuq6uHAsPLXGc655Hl
"""

"""
Bagging Classifier (From Scratch)

Implements a Bagging ensemble classifier by training multiple decision trees
on bootstrap samples of the dataset. Compatible with scikit-learn API.
"""

from sklearn.base import BaseEstimator, ClassifierMixin, clone
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.utils.estimator_checks import _name_estimators
import numpy as np

class BaggingFromScratch(BaseEstimator, ClassifierMixin):
    def __init__(self, n_estimators=10, criterion='entropy', vote='classlabel', weight=None, random_state=1):
        self.n_estimators = n_estimators
        self.criterion = criterion
        self.vote = vote
        self.weight = weight
        self.random_state = random_state

        ds_tree = DecisionTreeClassifier(criterion=criterion, random_state=random_state, max_depth=None)
        self.classifiers = [clone(ds_tree) for _ in range(n_estimators)]
        self.named_classifiers = {key: value for key, value in _name_estimators(self.classifiers)}

    def fit(self, x, y):
        if self.vote not in ('probability', 'classlabel'):
            raise ValueError(f"vote must be 'probability' or 'classlabel', got {self.vote}")
        if self.weight and len(self.weight) != len(self.classifiers):
            raise ValueError(f"Number of weights must match number of classifiers. "
                             f"Got {len(self.weight)} weights, {len(self.classifiers)} classifiers.")

        self.labelenc_ = LabelEncoder().fit(y)
        y_encoded = self.labelenc_.transform(y)
        self.classes_ = self.labelenc_.classes_
        self.classifiers_trained = []

        for clf in self.classifiers:
            bootstrap_indices = np.random.choice(x.shape[0], size=x.shape[0], replace=True)
            x_boot, y_boot = x[bootstrap_indices], y_encoded[bootstrap_indices]
            self.classifiers_trained.append(clone(clf).fit(x_boot, y_boot))
        return self

    def predict(self, x):
        if self.vote == 'probability':
            return np.argmax(self.predict_proba(x), axis=1)
        else:
            preds = np.array([clf.predict(x) for clf in self.classifiers_trained])
            weighted_votes = np.apply_along_axis(
                lambda x: np.bincount(x, weights=self.weight, minlength=len(self.classes_)),
                axis=0, arr=preds)
            maj_vote = np.argmax(weighted_votes, axis=0)
            return self.labelenc_.inverse_transform(maj_vote)

    def predict_proba(self, x):
        probas = np.array([clf.predict_proba(x) for clf in self.classifiers_trained])
        return np.average(probas, axis=0, weights=self.weight)

    def get_params(self, deep=True):
        if not deep:
            return super().get_params(deep=False)
        out = self.named_classifiers.copy()
        for name, step in self.named_classifiers.items():
            for key, value in step.get_params(deep=True).items():
                out[f'{name}__{key}'] = value
        return out