# -*- coding: utf-8 -*-
"""perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBkRx3zcZqLB5_Zuq6uHAsPLXGc655Hl
"""

import numpy as np

class MultiClassLogisticRegression:
    def __init__(self, n_iter=10, penalty='l2', eta=0.1, C=1.0, eps=1e-15,
                 early_stopping=5, random_state=1):
        self.n_iter = n_iter                  # Number of training epochs
        self.eta = eta                        # Learning rate
        self.penalty = penalty                # Type of regularization: 'l1', 'l2', or None
        self.C = C                            # Inverse regularization strength (like sklearn)
        self.eps = eps                        # Epsilon for numerical stability in log
        self.early_stopping = early_stopping  # Max consecutive non-improving epochs
        self.random_state = random_state      # Seed for reproducibility

    def fit(self, x, y):
        batch, input_dim = x.shape
        self.num_classes = len(np.unique(y))

        assert batch == len(y), "The number of samples in X and y must match."
        assert self.penalty in ('l1', 'l2', None), "Penalty must be 'l1', 'l2', or None."

        rng = np.random.RandomState(seed=self.random_state)

        # Initialize weights and bias for each class
        self.w_ = rng.normal(loc=0.0, scale=0.1, size=(self.num_classes, input_dim))
        self.b_ = np.zeros(self.num_classes)
        self.losses_ = []

        no_improve_epochs = 0

        for epoch in range(self.n_iter):
            # Net input: (batch_size, num_classes)
            net_sum = self.net_input(x)

            # Softmax activation per class
            net_act = self.activation(net_sum)  # Shape: (batch_size, num_classes)

            # One-hot encode targets: shape becomes (batch_size, num_classes)
            y_one_hot = np.eye(self.num_classes)[y]

            # Compute gradient: shape (batch_size, num_classes)
            error = y_one_hot - net_act

            # Update biases: shape (num_classes,)
            self.b_ += self.eta * error.mean(axis=0)

            # Update weights: shape (num_classes, input_dim)
            if self.penalty == 'l2':
                self.w_ += self.eta * error.T.dot(x) / batch - 2 * self.eta * (1/self.C) * self.w_
            elif self.penalty == 'l1':
                self.w_ += self.eta * error.T.dot(x) / batch - self.eta * (1/self.C) * np.sign(self.w_)
            else:
                self.w_ += self.eta * error.T.dot(x) / batch

            # Compute loss and check for early stopping
            loss = self.loss_fn(x, y, net_act)
            self.losses_.append(loss)

            if len(self.losses_) > 1 and abs(self.losses_[-2] - self.losses_[-1]) < 1e-6:
                no_improve_epochs += 1
            else:
                no_improve_epochs = 0

            if no_improve_epochs > self.early_stopping:
                print(f"Early stopping at epoch {epoch+1}")
                break

        return self

    def net_input(self, x):
        # Linear layer: returns (batch_size, num_classes)
        return x.dot(self.w_.T) + self.b_

    def activation(self, z):
        # Stable softmax: subtract max for numerical stability
        z -= np.max(z, axis=1, keepdims=True)
        exp_z = np.exp(z)
        return exp_z / np.sum(exp_z, axis=1, keepdims=True)

    def predict(self, x):
        # Return predicted class indices (argmax of probabilities)
        return np.argmax(self.activation(self.net_input(x)), axis=1)

    def predict_proba(self, x):
        # Return class probabilities
        return self.activation(self.net_input(x))

    def loss_fn(self, x, y, net_activation=None):
        if net_activation is None:
            net_activation = self.activation(self.net_input(x))

        # One-hot encode targets
        y_one_hot = np.eye(self.num_classes)[y]

        # Clip to prevent log(0) instability
        y_one_hot = np.clip(y_one_hot, self.eps, 1 - self.eps)
        net_activation = np.clip(net_activation, self.eps, 1 - self.eps)

        # Cross-entropy loss
        cross_entropy = -np.mean(np.sum(y_one_hot * np.log(net_activation), axis=1))

        # Regularization term
        if self.penalty == 'l2':
            reg = (1 / self.C) * np.sum(self.w_ ** 2)
        elif self.penalty == 'l1':
            reg = (1 / self.C) * np.sum(np.abs(self.w_))
        else:
            reg = 0.0

        return cross_entropy + reg

    def __repr__(self):
        return (f"MultiClassLogisticRegression(n_iter={self.n_iter}, penalty='{self.penalty}', "
                f"eta={self.eta}, C={self.C}, early_stopping={self.early_stopping}, "
                f"random_state={self.random_state})")

