# -*- coding: utf-8 -*-
"""perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBkRx3zcZqLB5_Zuq6uHAsPLXGc655Hl
"""

# Logistic Regression from Scratch using Full-Batch Gradient Descent

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from utils import plot_decision_region


class LogisticRegression_:
    def __init__(self, n_iter=100, eta=0.01, penalty='l2', C=1.0,
                 eps=1e-15, early_stopping=5, random_state=1):
        """
        Logistic Regression classifier using batch gradient descent.

        Parameters:
        - n_iter (int): Max number of epochs
        - eta (float): Learning rate
        - penalty (str): 'l2', 'l1', or None for regularization
        - C (float): Inverse of regularization strength (like sklearn)
        - eps (float): Epsilon for numerical stability in log
        - early_stopping (int): Stop training if loss stops improving after N epochs
        - random_state (int): Seed for reproducibility
        """
        self.n_iter = n_iter
        self.eta = eta
        self.penalty = penalty
        self.C = C
        self.eps = eps
        self.early_stopping = early_stopping
        self.random_state = random_state

    def fit(self, X, y):
        """
        Train the model using full-batch gradient descent.

        Parameters:
        - X (ndarray): Input features
        - y (ndarray): Binary target values (0 or 1)

        Returns:
        - self: fitted model
        """
        n_samples, n_features = X.shape
        assert len(y) == n_samples, "Mismatch between samples and labels"
        assert self.penalty in ('l1', 'l2', None), "penalty must be 'l1', 'l2' or None"

        rng = np.random.RandomState(self.random_state)
        self.w_ = rng.normal(loc=0.0, scale=0.1, size=n_features)
        self.b_ = 0.0
        self.losses_ = []

        no_improve_epochs = 0

        for epoch in range(self.n_iter):
            z = self.net_input(X)
            y_hat = self.activation(z)

            # Gradient updates
            self.b_ += self.eta * (y - y_hat).mean()

            grad_w = (y - y_hat).dot(X) / n_samples
            if self.penalty == 'l2':
                grad_w -= (2 * self.eta * (1 / self.C) * self.w_)
            elif self.penalty == 'l1':
                grad_w -= (self.eta * (1 / self.C) * np.sign(self.w_))

            self.w_ += self.eta * grad_w

            # Compute loss and check early stopping
            current_loss = self.loss_fn(X, y, y_hat)
            self.losses_.append(current_loss)

            if len(self.losses_) > 1 and abs(self.losses_[-2] - self.losses_[-1]) < 1e-6:
                no_improve_epochs += 1
            else:
                no_improve_epochs = 0

            if no_improve_epochs > self.early_stopping:
                break

        return self

    def net_input(self, X):
        """Compute linear combination (z = Xw + b)."""
        return np.dot(X, self.w_) + self.b_

    def activation(self, z):
        """Apply sigmoid activation with clipping."""
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))

    def predict_proba(self, X):
        """Predict probability estimates for input."""
        return self.activation(self.net_input(X))

    def predict(self, X):
        """Predict binary class labels."""
        return np.where(self.predict_proba(X) >= 0.5, 1, 0)

    def loss_fn(self, X, y, y_hat=None):
        """Compute binary cross-entropy loss with regularization."""
        if y_hat is None:
            y_hat = self.predict_proba(X)

        y_hat = np.clip(y_hat, self.eps, 1 - self.eps)
        loss = -(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)).mean()

        if self.penalty == 'l2':
            reg = (1 / self.C) * np.sum(self.w_ ** 2)
        elif self.penalty == 'l1':
            reg = (1 / self.C) * np.sum(np.abs(self.w_))
        else:
            reg = 0.0

        return loss + reg

    def __repr__(self):
        return (f"LogisticRegression(n_iter={self.n_iter}, penalty='{self.penalty}', "
                f"eta={self.eta}, C={self.C}, early_stopping={self.early_stopping}, "
                f"random_state={self.random_state})")

    # === Data Preparation ===
iris = load_iris()
mask = (iris.target == 0) | (iris.target == 1)
X = iris.data[mask][:, [0, 2]]  # Use sepal length and petal length
y = iris.target[mask]

# Shuffle data
perm = np.random.permutation(len(X))
X, Y = X[perm], y[perm]

# === Training Before Standardization ===
lrg1 = LogisticRegression_(eta=0.01, n_iter=15).fit(X, Y)
lrg2 = LogisticRegression_(eta=0.0001, n_iter=15).fit(X, Y)

# === Plotting ===
plt.figure(figsize=(14, 10))

# Decision boundary before standardization
plt.subplot(2, 3, 1)
plot_decision_region(X, Y, lrg1)
plt.title("Before Standardization")
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Petal Length (cm)')
plt.legend()

# Log loss (eta = 0.01)
plt.subplot(2, 3, 2)
plt.plot(range(1, len(lrg1.losses_) + 1), np.log10(lrg1.losses_), marker='o')
plt.title('LogReg - Learning Rate 0.01')
plt.xlabel('Epochs')
plt.ylabel('log(Loss)')

# Loss (eta = 0.0001)
plt.subplot(2, 3, 3)
plt.plot(range(1, len(lrg2.losses_) + 1), lrg2.losses_, marker='o')
plt.title('LogReg - Learning Rate 0.0001')
plt.xlabel('Epochs')
plt.ylabel('Loss')

# === Standardize Input ===
X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)
lrg1_sc = LogisticRegression_(eta=0.01, n_iter=15).fit(X_scaled, Y)
lrg2_sc = LogisticRegression_(eta=0.0001, n_iter=15).fit(X_scaled, Y)

# Decision boundary after standardization
plt.subplot(2, 3, 4)
plot_decision_region(X_scaled, Y, lrg1_sc)
plt.title("After Standardization")
plt.xlabel('Sepal Length (std)')
plt.ylabel('Petal Length (std)')
plt.legend()

# Log loss (standardized input, eta = 0.01)
plt.subplot(2, 3, 5)
plt.plot(range(1, len(lrg1_sc.losses_) + 1), np.log10(lrg1_sc.losses_), marker='o')
plt.title('Standardized - Learning Rate 0.01')
plt.xlabel('Epochs')
plt.ylabel('log(Loss)')

# Loss (standardized input, eta = 0.0001)
plt.subplot(2, 3, 6)
plt.plot(range(1, len(lrg2_sc.losses_) + 1), lrg2_sc.losses_, marker='o')
plt.title('Standardized - Learning Rate 0.0001')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.tight_layout()
plt.show()

# === Effect of Training Duration ===
plt.figure(figsize=(12, 4))
for i, n_iter in enumerate([15, 60, 140]):
    model = LogisticRegression_(eta=0.01, n_iter=n_iter).fit(X, Y)
    plt.subplot(1, 3, i + 1)
    plot_decision_region(X, Y, model)
    plt.title(f"Epochs = {n_iter}")
    if i == 0:
        plt.xlabel('Sepal Length (cm)')
        plt.ylabel('Petal Length (cm)')
        plt.legend()

plt.tight_layout()
plt.show()

